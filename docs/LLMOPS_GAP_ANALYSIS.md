# LLMOps èƒ½åŠ›å·®è·åˆ†æ

æœ¬æ–‡æ¡£å¯¹æ¯”é¡¶çº§å‚å•†æœŸæœ›çš„ LLMOps èƒ½åŠ›ä¸å½“å‰å®ç°çš„å·®è·ã€‚

---

## ğŸ“Š å®Œæ•´å¯¹æ¯”è¡¨

| LLMOps èƒ½åŠ› | å½“å‰å®ç° | çŠ¶æ€ | å·®è· | ä¼˜å…ˆçº§ |
|------------|---------|------|------|--------|
| **1. Observability** | åŸºç¡€ logging | ğŸŸ¡ 20% | ç¼ºå°‘ç»“æ„åŒ–æ—¥å¿—ã€è¿½è¸ªIDã€metricså¯¼å‡º | P0 |
| **2. Tracing** | Debug CLIè¿½è¸ª | ğŸŸ¡ 30% | æœ‰æœ¬åœ°èŠ‚ç‚¹è¿½è¸ªï¼Œä½†ç¼ºç”Ÿäº§çº§spanã€æŒä¹…åŒ–ã€å¯è§†åŒ– | P0 |
| **3. Monitoring** | Evalæ—¶ç›‘æ§ | ğŸŸ¡ 30% | ä»…ç¦»çº¿è¯„ä¼°ï¼Œæ— å®æ—¶ç›‘æ§ã€å‘Šè­¦ | P0 |
| **4. Caching** | æ—  | ğŸ”´ 0% | ç¼ºå°‘LLMå“åº”ç¼“å­˜ã€å‘é‡æ£€ç´¢ç¼“å­˜ | P1 |
| **5. Rate Limiting** | æ—  | ğŸ”´ 0% | ç¼ºå°‘APIé™æµã€tokené™é€Ÿã€quotaç®¡ç† | P1 |
| **6. Prompt Versioning** | æ—  | ğŸ”´ 0% | Prompt hardcodeï¼Œæ— ç‰ˆæœ¬ç®¡ç†ã€A/Bæµ‹è¯• | P1 |
| **7. Rollout** | æ—  | ğŸ”´ 0% | ç¼ºå°‘é‡‘ä¸é›€å‘å¸ƒã€è“ç»¿éƒ¨ç½²ã€æµé‡åˆ‡åˆ† | P2 |
| **8. Regression** | æ‰‹åŠ¨è¯„ä¼° | ğŸŸ¡ 40% | æœ‰è¯„ä¼°æ¡†æ¶ä½†æ— CIé›†æˆã€è‡ªåŠ¨baselineå¯¹æ¯” | P0 |
| **9. Timeout** | åŸºç¡€å®ç° | ğŸŸ¢ 70% | APIæœ‰60s timeoutï¼ŒHTTPæœ‰è¶…æ—¶è®¾ç½® | - |
| **10. Failure Isolation** | åŸºç¡€ try-catch | ğŸŸ¡ 40% | æœ‰å¼‚å¸¸å¤„ç†ä½†ç¼ºå°‘ç†”æ–­ã€é™çº§ã€é‡è¯• | P1 |

**å›¾ä¾‹**ï¼šğŸŸ¢ 70%+ | ğŸŸ¡ 30-70% | ğŸ”´ 0-30%

---

## ğŸ” è¯¦ç»†åˆ†æ

### 1ï¸âƒ£ **Observability (å¯è§‚æµ‹æ€§)** ğŸŸ¡ 20%

#### å½“å‰å®ç°
```python
# evaluation.py
import logging
logging.getLogger("asyncio").setLevel(logging.ERROR)

# api.py
logger = logging.getLogger("uvicorn.error")
logger.exception("Internal error in /v1/chat")
```

**æœ‰**ï¼š
- âœ… åŸºç¡€ Python logging
- âœ… Evalæ—¶è®°å½•æ€§èƒ½æŒ‡æ ‡ï¼ˆlatency, tokens, success rateï¼‰

**ç¼ºå¤±**ï¼š
- âŒ **ç»“æ„åŒ–æ—¥å¿—**ï¼ˆJSONæ ¼å¼ï¼Œä¾¿äºè§£æï¼‰
- âŒ **è¯·æ±‚è¿½è¸ªID**ï¼ˆtrace_id, span_idï¼‰
- âŒ **å…³é”®äº‹ä»¶æ—¥å¿—**ï¼š
  - æ¯æ¬¡LLMè°ƒç”¨ï¼ˆprompt, response, latency, tokensï¼‰
  - æ¯æ¬¡æ£€ç´¢ï¼ˆquery, top-k results, relevance scoresï¼‰
  - æ¯æ¬¡è·¯ç”±å†³ç­–ï¼ˆroute taken, confidenceï¼‰
- âŒ **æ—¥å¿—èšåˆ**ï¼ˆELK, Loki, CloudWatchï¼‰

#### é¡¶çº§å‚å•†æ ‡å‡†
```python
# ç¤ºä¾‹ï¼šç»“æ„åŒ–æ—¥å¿—
logger.info(
    "llm_call_complete",
    extra={
        "trace_id": "abc123",
        "span_id": "xyz789",
        "model": "gemini-2.5-flash",
        "prompt_tokens": 256,
        "completion_tokens": 128,
        "latency_ms": 1234,
        "cost_usd": 0.0045,
        "user_id": "user_456",
        "session_id": "session_789"
    }
)
```

---

### 2ï¸âƒ£ **Tracing (åˆ†å¸ƒå¼è¿½è¸ª)** ğŸŸ¡ 30%

#### å½“å‰å®ç°
```python
# debug_cli.py - æœ¬åœ°è°ƒè¯•è¿½è¸ª
for chunk in graph.stream(initial_state):
    for node, update in chunk.items():
        print(f"--- Node: {node} ---")  # æ˜¾ç¤ºæ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œ
        if "messages" in update:
            # æ‰“å°å·¥å…·è°ƒç”¨å’Œå†…å®¹
```

**æœ‰**ï¼š
- âœ… **èŠ‚ç‚¹çº§åˆ«è¿½è¸ª**ï¼ˆLangGraph streamè¾“å‡ºï¼‰
- âœ… **æœ¬åœ°è°ƒè¯•å¯è§†åŒ–**ï¼ˆCLIæ˜¾ç¤ºæ‰§è¡Œæµç¨‹ï¼‰
- âœ… **å·¥å…·è°ƒç”¨è¿½è¸ª**ï¼ˆæ˜¾ç¤ºå“ªä¸ªtoolè¢«è°ƒç”¨ï¼‰

**ä½†è¿™æ˜¯"Debug Tracing"ï¼Œä¸æ˜¯"Production Tracing"**

#### ä¸¤è€…çš„åŒºåˆ«

| ç‰¹æ€§ | Debug Tracing (ä½ çš„å®ç°) | Production Tracing (LLMOpsæ ‡å‡†) |
|------|------------------------|-------------------------------|
| **ç›®çš„** | æœ¬åœ°å¼€å‘è°ƒè¯• | ç”Ÿäº§ç¯å¢ƒå¯è§‚æµ‹æ€§ |
| **è¾“å‡º** | ç»ˆç«¯ stdout | ç»“æ„åŒ–æ—¥å¿— + è¿½è¸ªç³»ç»Ÿ |
| **æŒä¹…åŒ–** | æ— ï¼ˆæ‰“å°å³æ¶ˆå¤±ï¼‰ | æ°¸ä¹…å­˜å‚¨ï¼ˆæ•°æ®åº“/æ—¥å¿—ç³»ç»Ÿï¼‰ |
| **æŸ¥è¯¢èƒ½åŠ›** | æ— æ³•å›æº¯å†å² | å¯æœç´¢å†å²trace |
| **æ€§èƒ½åˆ†æ** | æ— æ—¶é—´æˆ³/å»¶è¿Ÿ | ç²¾ç¡®çš„spanæ—¶é—´ + ç«ç„°å›¾ |
| **è·¨æœåŠ¡** | å•è¿›ç¨‹å†… | è·¨å¤šä¸ªæœåŠ¡è¿½è¸ª |
| **ç”Ÿäº§å¯ç”¨** | âŒ æ€§èƒ½å¼€é”€å¤§ | âœ… ä½å¼€é”€å¼‚æ­¥å¯¼å‡º |
| **å‘Šè­¦é›†æˆ** | âŒ | âœ… å¯è§¦å‘å‘Šè­¦ |

#### ä»ç„¶ç¼ºå¤±çš„èƒ½åŠ›
- âŒ **OpenTelemetry é›†æˆ**ï¼ˆå·¥ä¸šæ ‡å‡†ï¼‰
- âŒ **Span å±‚çº§ç»“æ„ + æ—¶é—´**ï¼š
  ```
  User Request (total: 5.2s, trace_id: abc123)
  â”œâ”€ generate_query_or_respond (50ms, span_id: xyz1)
  â”œâ”€ retrieve (200ms, span_id: xyz2)
  â”‚  â”œâ”€ embedding_call (80ms, span_id: xyz3)
  â”‚  â””â”€ vector_search (120ms, span_id: xyz4)
  â”œâ”€ grade_documents (300ms, span_id: xyz5)
  â”‚  â””â”€ llm_call_grading (280ms, span_id: xyz6, tokens: 512)
  â””â”€ generate_answer (800ms, span_id: xyz7)
     â””â”€ llm_call_generation (780ms, span_id: xyz8, tokens: 1024)
  ```
- âŒ **æŒä¹…åŒ–å­˜å‚¨**ï¼ˆElasticsearch, Tempoï¼‰
- âŒ **å¯è§†åŒ–å·¥å…·**ï¼ˆJaeger, Zipkin, Datadog APMï¼‰
- âŒ **è·¨è¯·æ±‚å…³è”**ï¼ˆåŒä¸€ç”¨æˆ·çš„å¤šæ¬¡è¯·æ±‚ï¼‰
- âŒ **æ€§èƒ½åˆ†æ**ï¼ˆå“ªä¸ªèŠ‚ç‚¹æœ€æ…¢ï¼Ÿå“ªä¸ªLLMè°ƒç”¨æœ€è´µï¼Ÿï¼‰

#### é¡¶çº§å‚å•†æ ‡å‡†
```python
from opentelemetry import trace
from opentelemetry.instrumentation.langchain import LangChainInstrumentor

tracer = trace.get_tracer(__name__)

with tracer.start_as_current_span("rag_pipeline") as span:
    span.set_attribute("question", question)
    
    with tracer.start_as_current_span("retrieval"):
        docs = retrieve(question)
        span.set_attribute("docs_count", len(docs))
    
    with tracer.start_as_current_span("generation"):
        answer = generate(docs, question)
        span.set_attribute("answer_length", len(answer))
```

---

### 3ï¸âƒ£ **Monitoring (å®æ—¶ç›‘æ§)** ğŸŸ¡ 30%

#### å½“å‰å®ç°
```python
# evaluation.py - ä»…åœ¨ç¦»çº¿è¯„ä¼°æ—¶
{
    'latency_p50': 6.87,
    'latency_p95': 10.04,
    'tokens_mean': 3562,
    'success_rate': 1.0
}
```

**æœ‰**ï¼š
- âœ… ç¦»çº¿è¯„ä¼°æ—¶æ”¶é›†æ€§èƒ½æŒ‡æ ‡
- âœ… ä¿å­˜åˆ° CSV/JSON

**ç¼ºå¤±**ï¼š
- âŒ **å®æ—¶ç›‘æ§**ï¼ˆç”Ÿäº§ç¯å¢ƒæ¯æ¬¡è¯·æ±‚ï¼‰
- âŒ **Metrics å¯¼å‡º**ï¼ˆPrometheus, StatsDï¼‰
- âŒ **Dashboard**ï¼ˆGrafana, Datadogï¼‰
- âŒ **å‘Šè­¦è§„åˆ™**ï¼š
  - Latency > p95 é˜ˆå€¼
  - Success rate < 95%
  - Cost > é¢„ç®—
  - RAGAS score ä¸‹é™
- âŒ **SLI/SLO**ï¼ˆæœåŠ¡æ°´å¹³æŒ‡æ ‡/ç›®æ ‡ï¼‰

#### é¡¶çº§å‚å•†æ ‡å‡†
```python
from prometheus_client import Counter, Histogram, Gauge

# Metricså®šä¹‰
llm_requests_total = Counter('llm_requests_total', 'Total LLM requests', ['model', 'status'])
llm_latency_seconds = Histogram('llm_latency_seconds', 'LLM latency', ['model'])
llm_cost_usd = Counter('llm_cost_usd_total', 'Total LLM cost', ['model'])
faithfulness_score = Gauge('rag_faithfulness_score', 'Current faithfulness score')

# ä½¿ç”¨
llm_requests_total.labels(model='gemini-2.5-flash', status='success').inc()
llm_latency_seconds.labels(model='gemini-2.5-flash').observe(1.234)
llm_cost_usd.labels(model='gemini-2.5-flash').inc(0.0045)
```

---

### 4ï¸âƒ£ **Caching (ç¼“å­˜)** ğŸ”´ 0%

#### å½“å‰å®ç°
**å®Œå…¨ç¼ºå¤±**

#### ç¼ºå¤±çš„èƒ½åŠ›
- âŒ **LLM å“åº”ç¼“å­˜**ï¼š
  - ç›¸åŒ/ç›¸ä¼¼é—®é¢˜ç›´æ¥è¿”å›ç¼“å­˜ç­”æ¡ˆ
  - èŠ‚çœæˆæœ¬ï¼ˆGemini: $0.075/1M tokensï¼‰
  - é™ä½å»¶è¿Ÿ
- âŒ **å‘é‡æ£€ç´¢ç¼“å­˜**ï¼š
  - ç¼“å­˜çƒ­é—¨æŸ¥è¯¢çš„æ£€ç´¢ç»“æœ
  - é¿å…é‡å¤embeddingè®¡ç®—
- âŒ **è¯­ä¹‰ç¼“å­˜**ï¼š
  - ç›¸ä¼¼é—®é¢˜å‘½ä¸­ç¼“å­˜ï¼ˆembeddingç›¸ä¼¼åº¦ï¼‰
- âŒ **TTL ç­–ç•¥**ï¼ˆtime-to-liveï¼‰
- âŒ **ç¼“å­˜å¤±æ•ˆç­–ç•¥**ï¼ˆçŸ¥è¯†åº“æ›´æ–°åï¼‰

#### é¡¶çº§å‚å•†æ ‡å‡†
```python
from langchain.cache import InMemoryCache, RedisCache
from langchain.globals import set_llm_cache

# æ–¹æ¡ˆ1: å†…å­˜ç¼“å­˜ï¼ˆç®€å•ï¼‰
set_llm_cache(InMemoryCache())

# æ–¹æ¡ˆ2: Redisç¼“å­˜ï¼ˆç”Ÿäº§ï¼‰
set_llm_cache(RedisCache(redis_url="redis://localhost:6379"))

# æ–¹æ¡ˆ3: è¯­ä¹‰ç¼“å­˜ï¼ˆé«˜çº§ï¼‰
from langchain.cache import GPTCache
set_llm_cache(GPTCache(similarity_threshold=0.9))

# ä½¿ç”¨åè‡ªåŠ¨ç¼“å­˜
answer = llm.invoke(question)  # é¦–æ¬¡ï¼šè°ƒç”¨API
answer = llm.invoke(question)  # ç¬¬äºŒæ¬¡ï¼šä»ç¼“å­˜è¿”å›
```

**æˆæœ¬èŠ‚çœç¤ºä¾‹**ï¼š
- 30ä¸ªevalé—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜é‡å¤è¯„ä¼°10æ¬¡
- æ— ç¼“å­˜ï¼š300æ¬¡LLMè°ƒç”¨ Ã— $0.01 = $3.00
- æœ‰ç¼“å­˜ï¼š30æ¬¡LLMè°ƒç”¨ Ã— $0.01 = $0.30
- **èŠ‚çœ 90%**

---

### 5ï¸âƒ£ **Rate Limiting (é€Ÿç‡é™åˆ¶)** ğŸ”´ 0%

#### å½“å‰å®ç°
**å®Œå…¨ç¼ºå¤±**

#### ç¼ºå¤±çš„èƒ½åŠ›
- âŒ **API é™æµ**ï¼š
  - æ¯ç”¨æˆ·/æ¯IPè¯·æ±‚é™åˆ¶
  - é˜²æ­¢æ»¥ç”¨
- âŒ **Token é™é€Ÿ**ï¼š
  - æ¯ç”¨æˆ·token quota
  - æˆæœ¬æ§åˆ¶
- âŒ **å¹¶å‘æ§åˆ¶**ï¼š
  - åŒæ—¶è¿›è¡Œçš„LLMè°ƒç”¨æ•°
  - é¿å…è¶…å‡ºprovideré™åˆ¶
- âŒ **ä¼˜é›…é™çº§**ï¼š
  - è¾¾åˆ°é™åˆ¶åè¿”å›å‹å¥½é”™è¯¯
  - æ’é˜Ÿæœºåˆ¶

#### é¡¶çº§å‚å•†æ ‡å‡†
```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.post("/v1/chat")
@limiter.limit("10/minute")  # æ¯åˆ†é’Ÿ10æ¬¡
async def chat(request: Request):
    # è‡ªåŠ¨é™æµ
    pass

# Token quotaç®¡ç†
from redis import Redis
redis_client = Redis()

def check_token_quota(user_id: str, tokens: int) -> bool:
    key = f"quota:{user_id}:{date.today()}"
    used = redis_client.get(key) or 0
    if int(used) + tokens > 100000:  # æ¯æ—¥10ä¸‡tokens
        return False
    redis_client.incrby(key, tokens)
    return True
```

---

### 6ï¸âƒ£ **Prompt Versioning (Prompt ç‰ˆæœ¬ç®¡ç†)** ğŸ”´ 0%

#### å½“å‰å®ç°
```python
# nodes.py - Prompt hardcodeåœ¨ä»£ç é‡Œ
system_message = """You are an expert assistant specializing in analyzing blog posts...
Provide detailed, accurate information based strictly on the retrieved documents..."""
```

**é—®é¢˜**ï¼š
- âŒ Promptå˜æ›´éœ€è¦ä¿®æ”¹ä»£ç ã€é‡æ–°éƒ¨ç½²
- âŒ æ— æ³•å¿«é€Ÿå›æ»šåˆ°ä¹‹å‰çš„prompt
- âŒ æ— æ³•A/Bæµ‹è¯•ä¸åŒprompt
- âŒ æ— æ³•è¿½è¸ªpromptå˜æ›´å†å²

#### ç¼ºå¤±çš„èƒ½åŠ›
- âŒ **Prompt æ¨¡æ¿ç®¡ç†**ï¼ˆå¤–éƒ¨é…ç½®æ–‡ä»¶ï¼‰
- âŒ **ç‰ˆæœ¬æ§åˆ¶**ï¼š
  ```
  prompts/
  â”œâ”€â”€ answer_generation_v1.txt
  â”œâ”€â”€ answer_generation_v2.txt
  â””â”€â”€ answer_generation_v3.txt (current)
  ```
- âŒ **A/B æµ‹è¯•**ï¼š
  - 50%æµé‡ç”¨v2ï¼Œ50%ç”¨v3
  - å¯¹æ¯”RAGASæŒ‡æ ‡
- âŒ **Prompt Registry**ï¼ˆä¸­å¿ƒåŒ–ç®¡ç†ï¼‰
- âŒ **å˜æ›´å®¡è®¡æ—¥å¿—**

#### é¡¶çº§å‚å•†æ ‡å‡†
```python
# æ–¹æ¡ˆ1: LangSmith / PromptLayer
from langsmith import Client
client = Client()

# è·å–æœ€æ–°ç‰ˆæœ¬prompt
prompt = client.pull_prompt("answer-generation", version="latest")

# æ–¹æ¡ˆ2: è‡ªå»ºPrompt Registry
from prompt_registry import PromptRegistry
registry = PromptRegistry(backend="redis")

prompt = registry.get(
    name="answer_generation",
    version="v3",
    fallback_version="v2"  # å›æ»šèƒ½åŠ›
)

# æ–¹æ¡ˆ3: Feature Flag + A/Bæµ‹è¯•
if user_id % 2 == 0:
    prompt = registry.get("answer_generation", version="v3")
else:
    prompt = registry.get("answer_generation", version="v2")
```

---

### 7ï¸âƒ£ **Rollout (å‘å¸ƒç­–ç•¥)** ğŸ”´ 0%

#### å½“å‰å®ç°
**å®Œå…¨ç¼ºå¤±**

#### ç¼ºå¤±çš„èƒ½åŠ›
- âŒ **é‡‘ä¸é›€å‘å¸ƒ** (Canary Deployment)ï¼š
  - 5%æµé‡ â†’ æ–°ç‰ˆæœ¬
  - ç›‘æ§æŒ‡æ ‡ â†’ å¦‚æœOKï¼Œé€æ­¥æ‰©å¤§åˆ°100%
- âŒ **è“ç»¿éƒ¨ç½²** (Blue-Green Deployment)ï¼š
  - éƒ¨ç½²æ–°ç‰ˆæœ¬ï¼ˆGreenï¼‰
  - æµé‡ä¸€æ¬¡æ€§åˆ‡æ¢
  - å¿«é€Ÿå›æ»š
- âŒ **A/B æµ‹è¯•**ï¼š
  - å¯¹æ¯”ä¸åŒæ¨¡å‹ï¼ˆGemini vs GPT-4ï¼‰
  - å¯¹æ¯”ä¸åŒæ£€ç´¢ç­–ç•¥
- âŒ **Feature Flags**ï¼š
  - åŠ¨æ€å¼€å…³åŠŸèƒ½ï¼ˆæ— éœ€é‡æ–°éƒ¨ç½²ï¼‰

#### é¡¶çº§å‚å•†æ ‡å‡†
```python
# ç¤ºä¾‹ï¼šé‡‘ä¸é›€å‘å¸ƒ
from launchdarkly import LDClient

ld_client = LDClient(sdk_key="your-key")

def get_model_version(user_id: str) -> str:
    """æ ¹æ®feature flagè¿”å›æ¨¡å‹ç‰ˆæœ¬"""
    return ld_client.variation(
        "model-version",
        {"key": user_id},
        default="v1"  # é»˜è®¤ç¨³å®šç‰ˆæœ¬
    )

# ä½¿ç”¨
model_version = get_model_version(user_id)
if model_version == "v2":
    model = "gemini-2.5-flash"  # æ–°ç‰ˆæœ¬ï¼ˆ5%æµé‡ï¼‰
else:
    model = "gemini-2.0-flash"  # æ—§ç‰ˆæœ¬ï¼ˆ95%æµé‡ï¼‰
```

**å‘å¸ƒæµç¨‹**ï¼š
```
1. éƒ¨ç½² v2ï¼ˆ5% æµé‡ï¼‰
2. ç›‘æ§ 24hï¼š
   - RAGASæŒ‡æ ‡ >= v1
   - Latency <= v1 + 10%
   - æ— é”™è¯¯é£™å‡
3. å¦‚æœOK â†’ æ‰©å¤§åˆ° 25%
4. é‡å¤ç›‘æ§ â†’ 50% â†’ 100%
5. å¦‚æœANYé—®é¢˜ â†’ ç«‹å³å›æ»šåˆ° v1
```

---

### 8ï¸âƒ£ **Regression (å›å½’æµ‹è¯•)** ğŸŸ¡ 40%

#### å½“å‰å®ç°
```python
# evaluation.py - æ‰‹åŠ¨è¿è¡Œ
python run_evaluation.py

# è¾“å‡º
{
  "faithfulness": 0.954,
  "hit@3_mean": 1.0,
  "mrr_mean": 0.807
}
```

**æœ‰**ï¼š
- âœ… å®Œæ•´çš„è¯„ä¼°æ¡†æ¶ï¼ˆRAGAS + æ£€ç´¢ç¡¬æŒ‡æ ‡ï¼‰
- âœ… Golden dataset (30é—®é¢˜)
- âœ… æ€§èƒ½æŒ‡æ ‡æ”¶é›†

**ç¼ºå¤±**ï¼š
- âŒ **CI/CD é›†æˆ**ï¼ˆGitHub Actions, GitLab CIï¼‰
- âŒ **è‡ªåŠ¨baselineå¯¹æ¯”**ï¼š
  ```python
  # ä¼ªä»£ç 
  baseline = load_baseline("baseline_v1.json")
  current = run_evaluation()
  
  assert current.faithfulness >= baseline.faithfulness - 0.05, "Regression!"
  assert current.hit_at_3 >= baseline.hit_at_3 - 0.10, "Regression!"
  ```
- âŒ **å˜æ›´æ£€æµ‹å‘Šè­¦**ï¼ˆæ€§èƒ½ä¸‹é™>5%å‘é‚®ä»¶/Slackï¼‰
- âŒ **å†å²è¶‹åŠ¿è¿½è¸ª**ï¼ˆæ¯æ¬¡commitçš„æŒ‡æ ‡å˜åŒ–ï¼‰
- âŒ **è‡ªåŠ¨å›æ»šè§¦å‘**ï¼ˆå¦‚æœregressionä¸¥é‡ï¼‰

#### é¡¶çº§å‚å•†æ ‡å‡†
```yaml
# .github/workflows/eval-regression.yml
name: Evaluation Regression Tests

on: [push, pull_request]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Run Evaluation
        run: python evaluation/run_evaluation.py
      
      - name: Compare with Baseline
        run: python tests/regression_check.py --baseline baseline.json --strict
      
      - name: Fail if Regression
        run: |
          if [ $? -ne 0 ]; then
            echo "âŒ Regression detected! Blocking merge."
            exit 1
          fi
      
      - name: Upload Results
        uses: actions/upload-artifact@v2
        with:
          name: eval-results
          path: eval_results/
```

---

### 9ï¸âƒ£ **Timeout** ğŸŸ¢ 70%

#### å½“å‰å®ç°
```python
# api.py
timeout_s: int = Field(default=60, ge=1)
timeout=req.timeout_s

# loader.py
resp = requests.get(item, timeout=(10, 30))  # (connect, read)
```

**æœ‰**ï¼š
- âœ… API endpointæœ‰60s timeout
- âœ… HTTPè¯·æ±‚æœ‰timeoutï¼ˆ10s connect, 30s readï¼‰
- âœ… asyncio.TimeoutErrorå¤„ç†

**æ”¹è¿›ç©ºé—´**ï¼š
- âš ï¸ ç²’åº¦ä¸å¤Ÿç»†ï¼š
  - LLMè°ƒç”¨timeoutåº”è¯¥ç‹¬ç«‹é…ç½®
  - æ£€ç´¢timeoutåº”è¯¥ç‹¬ç«‹é…ç½®
  - ä¸åŒstageä¸åŒtimeout
- âš ï¸ æ— timeout budgetç®¡ç†ï¼š
  - æ€»timeout = routing(5s) + retrieval(10s) + generation(30s)

#### é¡¶çº§å‚å•†æ ‡å‡†
```python
# ç»†ç²’åº¦timeouté…ç½®
class TimeoutConfig:
    routing_timeout_s: int = 5
    retrieval_timeout_s: int = 10
    grading_timeout_s: int = 15
    generation_timeout_s: int = 30
    total_timeout_s: int = 60  # æ€»é¢„ç®—

# ä½¿ç”¨
with timeout_context(TimeoutConfig.retrieval_timeout_s):
    docs = retriever.invoke(query)
```

---

### ğŸ”Ÿ **Failure Isolation (æ•…éšœéš”ç¦»)** ğŸŸ¡ 40%

#### å½“å‰å®ç°
```python
# evaluation.py
try:
    result = run_rag_pipeline_with_metrics(question, graph)
except Exception as e:
    print(f"âœ— Error: {e}")
    # ç»§ç»­ä¸‹ä¸€ä¸ªé—®é¢˜
```

**æœ‰**ï¼š
- âœ… åŸºç¡€ try-catch
- âœ… é”™è¯¯ç»§ç»­æ‰§è¡Œï¼ˆä¸ä¸­æ–­æ•´ä¸ªè¯„ä¼°ï¼‰
- âœ… è®°å½• success/failure

**ç¼ºå¤±**ï¼š
- âŒ **ç†”æ–­å™¨** (Circuit Breaker)ï¼š
  - è¿ç»­Næ¬¡å¤±è´¥ â†’ åœæ­¢è°ƒç”¨LLM
  - é¿å…é›ªå´©
- âŒ **é™çº§ç­–ç•¥** (Graceful Degradation)ï¼š
  - LLMä¸å¯ç”¨ â†’ è¿”å›æ¨¡æ¿ç­”æ¡ˆ
  - æ£€ç´¢å¤±è´¥ â†’ ä½¿ç”¨ç¼“å­˜ç»“æœ
- âŒ **é‡è¯•æœºåˆ¶** (Retry with Backoff)ï¼š
  - ä¸´æ—¶é”™è¯¯ â†’ è‡ªåŠ¨é‡è¯•3æ¬¡
  - æŒ‡æ•°é€€é¿
- âŒ **æ•…éšœè½¬ç§»** (Failover)ï¼š
  - Geminiä¸å¯ç”¨ â†’ åˆ‡æ¢åˆ°GPT-4
- âŒ **Bulkhead éš”ç¦»**ï¼š
  - ä¸åŒç”¨æˆ·/ç§Ÿæˆ·ä½¿ç”¨ç‹¬ç«‹èµ„æºæ± 

#### é¡¶çº§å‚å•†æ ‡å‡†
```python
from tenacity import retry, stop_after_attempt, wait_exponential
from pybreaker import CircuitBreaker

# 1. é‡è¯•æœºåˆ¶
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def call_llm(prompt: str):
    return llm.invoke(prompt)

# 2. ç†”æ–­å™¨
breaker = CircuitBreaker(
    fail_max=5,        # 5æ¬¡å¤±è´¥åæ–­å¼€
    timeout_duration=60 # 60såå°è¯•åŠå¼€
)

@breaker
def call_llm_with_breaker(prompt: str):
    return llm.invoke(prompt)

# 3. é™çº§
try:
    answer = call_llm_with_breaker(prompt)
except Exception:
    # é™çº§ï¼šè¿”å›æ¨¡æ¿ç­”æ¡ˆ
    answer = "I'm sorry, I cannot answer right now. Please try again later."
```

---

## ğŸ¯ ä¼˜å…ˆçº§æ’åº & å®ç°å»ºè®®

### P0 - å¿…é¡»å®ç°ï¼ˆé¢è¯•å¿…é—®ï¼‰

| èƒ½åŠ› | å·¥ä½œé‡ | ROI | å®ç°å»ºè®® |
|------|--------|-----|----------|
| **Observability** | 2å¤© | é«˜ | 1. æ·»åŠ ç»“æ„åŒ–JSONæ—¥å¿—<br>2. è®°å½•æ¯æ¬¡LLMè°ƒç”¨<br>3. æ·»åŠ trace_id |
| **Tracing** | 3å¤© | é«˜ | 1. é›†æˆOpenTelemetry<br>2. æ·»åŠ spanåˆ°å…³é”®å‡½æ•°<br>3. å¯¼å‡ºåˆ°Jaeger |
| **Monitoring** | 3å¤© | é«˜ | 1. æ·»åŠ Prometheus metrics<br>2. å®æ—¶æ”¶é›†latency/tokens/cost<br>3. ç®€å•Grafana dashboard |
| **Regression CI** | 2å¤© | é«˜ | 1. GitHub Actions workflow<br>2. Baseline comparison script<br>3. è‡ªåŠ¨fail on regression |

**æ€»å·¥ä½œé‡ï¼š~10å¤©**

### P1 - åº”è¯¥å®ç°ï¼ˆåŠ åˆ†é¡¹ï¼‰

| èƒ½åŠ› | å·¥ä½œé‡ | ROI | å®ç°å»ºè®® |
|------|--------|-----|----------|
| **Caching** | 2å¤© | ä¸­ | 1. LangChain InMemoryCache<br>2. è¯­ä¹‰ç¼“å­˜ï¼ˆå¯é€‰ï¼‰ |
| **Rate Limiting** | 1å¤© | ä¸­ | 1. SlowAPIé›†æˆ<br>2. æ¯ç”¨æˆ·é™æµ |
| **Prompt Versioning** | 2å¤© | ä¸­ | 1. Promptæ¨¡æ¿å¤–éƒ¨åŒ–<br>2. ç®€å•ç‰ˆæœ¬ç®¡ç† |
| **Failure Isolation** | 2å¤© | ä¸­ | 1. Tenacityé‡è¯•<br>2. PyBreakerç†”æ–­å™¨ |

**æ€»å·¥ä½œé‡ï¼š~7å¤©**

### P2 - å¯ä»¥æ¨è¿Ÿï¼ˆæœªæ¥ä¼˜åŒ–ï¼‰

| èƒ½åŠ› | å·¥ä½œé‡ | ROI | å®ç°å»ºè®® |
|------|--------|-----|----------|
| **Rollout** | 5å¤© | ä½ | 1. LaunchDarklyé›†æˆ<br>2. é‡‘ä¸é›€å‘å¸ƒæµç¨‹ |

---

## ğŸ“ å¿«é€Ÿèµ¢å¾—é¢è¯•å®˜çš„å®ç°

å¦‚æœåªæœ‰ **1å‘¨æ—¶é—´**ï¼Œå®ç°ä»¥ä¸‹"é¢å­å·¥ç¨‹"ï¼š

### Day 1-2: Observability + Tracing
```python
# æ·»åŠ åˆ° evaluation.py
import structlog
logger = structlog.get_logger()

def run_rag_pipeline_with_metrics(question: str, graph, trace_id: str):
    logger.info("rag_pipeline_start", trace_id=trace_id, question=question[:50])
    
    # ... existing code ...
    
    logger.info("rag_pipeline_complete", 
                trace_id=trace_id,
                latency_s=total_latency,
                tokens=estimated_tokens,
                success=success)
```

### Day 3: Monitoring
```python
# æ·»åŠ  metrics.py
from prometheus_client import Counter, Histogram
import time

llm_calls = Counter('llm_calls_total', 'Total LLM calls', ['model', 'status'])
llm_latency = Histogram('llm_latency_seconds', 'LLM latency')

def monitored_llm_call(llm, prompt):
    start = time.time()
    try:
        result = llm.invoke(prompt)
        llm_calls.labels(model='gemini-2.5-flash', status='success').inc()
        return result
    except Exception as e:
        llm_calls.labels(model='gemini-2.5-flash', status='error').inc()
        raise
    finally:
        llm_latency.observe(time.time() - start)
```

### Day 4: Caching
```python
# æ·»åŠ åˆ° graph_builder.py
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())  # ä¸€è¡Œä»£ç ï¼
```

### Day 5: Regression CI
```yaml
# .github/workflows/eval.yml
name: Eval on Push
on: [push]
jobs:
  eval:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - run: python evaluation/run_evaluation.py
      - run: python tests/check_regression.py
```

**é¢è¯•æ—¶è¯´**ï¼š
> "æˆ‘å®ç°äº†å®Œæ•´çš„LLMOps pipelineï¼š
> - **Observability**: ç»“æ„åŒ–æ—¥å¿—ï¼Œæ¯æ¬¡LLMè°ƒç”¨éƒ½æœ‰trace_id
> - **Monitoring**: Prometheus metricså®æ—¶ç›‘æ§latency/tokens/cost
> - **Caching**: LangChainç¼“å­˜ï¼Œå‡å°‘90%é‡å¤è°ƒç”¨æˆæœ¬
> - **Regression CI**: GitHub Actionsè‡ªåŠ¨æ£€æµ‹æ€§èƒ½å›å½’
> 
> æœªæ¥è®¡åˆ’æ·»åŠ ï¼šåˆ†å¸ƒå¼è¿½è¸ªï¼ˆOpenTelemetryï¼‰ã€é‡‘ä¸é›€å‘å¸ƒã€Promptç‰ˆæœ¬ç®¡ç†ã€‚"

---

## ğŸ† æ€»ç»“

### å½“å‰çŠ¶æ€ï¼š
- **å¼ºé¡¹**ï¼šæœ‰å®Œæ•´çš„ç¦»çº¿è¯„ä¼°æ¡†æ¶ã€åŸºç¡€timeout
- **å¼±é¡¹**ï¼šç¼ºå°‘ç”Ÿäº§çº§LLMOpsèƒ½åŠ›

### è¾¾åˆ°é¡¶çº§å‚å•†æ ‡å‡†éœ€è¦ï¼š
- **çŸ­æœŸ** (1-2å‘¨)ï¼šObservability + Monitoring + Caching + Regression CI
- **ä¸­æœŸ** (1ä¸ªæœˆ)ï¼šTracing + Rate Limiting + Prompt Versioning + Failure Isolation
- **é•¿æœŸ** (3ä¸ªæœˆ)ï¼šRollout + A/B Testing + å®Œæ•´çš„SREå®è·µ

### å…³é”®insightï¼š
**è¯„ä¼°æ˜¯å¿…è¦çš„ï¼Œä½†ä¸å……åˆ†ã€‚é¡¶çº§å‚å•†æœŸæœ›çš„æ˜¯"å¯è§‚æµ‹ã€å¯æ§åˆ¶ã€å¯ä¼˜åŒ–"çš„ç”Ÿäº§ç³»ç»Ÿã€‚**

*æœ€åæ›´æ–°: 2026-01-24*
